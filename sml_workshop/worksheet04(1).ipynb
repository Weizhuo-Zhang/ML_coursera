{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "worksheet04(1).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Weizhuo-Zhang/ML_coursera/blob/master/sml_workshop/worksheet04(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xtE6CEIW5OI",
        "colab_type": "text"
      },
      "source": [
        "# COMP90051 Workshop 4\n",
        "## Logistic regression\n",
        "***\n",
        "In this workshop we'll be implementing L2-regularised logistic regression using `scipy` and `numpy`. \n",
        "Our key objectives are:\n",
        "\n",
        "* to become familiar with the optimisation problem that sits behind L2-regularised logistic regression;\n",
        "* to apply polynomial basis expansion and recognise when it's useful; and\n",
        "* to experiment with the effect of L2 regularisation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgMu9wwJW5OJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU1l4dQ6W5OL",
        "colab_type": "text"
      },
      "source": [
        "### 1. Binary classification data\n",
        "Let's begin by generating some binary classification data.\n",
        "To make it easy for us to visualise the results, we'll stick to a two-dimensional feature space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtTnKLvUW5OL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FIGURE_RESOLUTION = 128\n",
        "plt.rcParams['figure.dpi'] = FIGURE_RESOLUTION\n",
        "\n",
        "from sklearn.datasets import make_circles\n",
        "X, Y = make_circles(n_samples=500, noise=0.17, factor=0.65, random_state=90051)\n",
        "plt.plot(X[Y==0,0], X[Y==0,1], '.', label = \"$y=0$\", c='r')\n",
        "plt.plot(X[Y==1,0], X[Y==1,1], '.', label = \"$y=1$\", c='g')\n",
        "plt.legend()\n",
        "plt.xlabel(\"$x_0$\")\n",
        "plt.ylabel(\"$x_1$\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgamDdH5W5ON",
        "colab_type": "text"
      },
      "source": [
        "**Question:** What's interesting about this data? Do you think logistic regression will perform well?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2h-2ahEW5OO",
        "colab_type": "text"
      },
      "source": [
        "In preparation for fitting and evaluating a logistic regression model, we randomly partition the data into train/test sets. We use the `train_test_split` function from `sklearn`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEycUe5AW5OO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=90051)\n",
        "print(\"Training set has {} instances. Test set has {} instances.\".format(X_train.shape[0], X_test.shape[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu-aW5rOW5OQ",
        "colab_type": "text"
      },
      "source": [
        "## 2. Logistic Regression\n",
        "In binary classification we receive training data $\\mathcal{D} = \\left((\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n, y_n)\\right)$, where $\\mathbf{x}_k \\in \\mathbb{R}^N$ denotes the feature vector associated with the $k$th training point and the targets $y \\in \\{0,1\\}$. Logistic regression models the distribution of the binary target $y$ *conditional* on the feature vector $\\mathbf{x}$ as\n",
        "\n",
        "\\begin{equation}\n",
        "y | \\mathbf{x} \\sim \\mathrm{Bernoulli}[\\sigma(\\mathbf{w}^T \\mathbf{x} + b)]\n",
        "\\end{equation}\n",
        "\n",
        "where $\\mathbf{w} \\in \\mathbb{R}^N$ is the weight vector, $b \\in \\mathbb{R}$ is the bias term and $\\sigma(z) = 1/(1 + e^{-z})$ is the logistic function. Note here our parameter of interest $\\theta$ is the conditional probability of a particular instance belonging to class 1 given observation of the associated feature vector $\\mathbf{x}$:\n",
        "\n",
        "$$\\theta = p(y = 1 \\vert \\mathbf{x}) = \\sigma\\left(\\mathbf{w}^T \\mathbf{x} + b\\right) $$\n",
        "To simplify the notation, we'll collect the model parameters $\\mathbf{w}$ and $b$ in a single vector $\\mathbf{v} = [b, \\mathbf{w}]$. \n",
        "\n",
        "To find appropriate parameters $\\mathbf{v}$, we want to maximize the log-likelihood with respect to $\\mathbf{v}$, in lecture it was shown this is equivalent to minimization of the sum of cross-entropies over the instances ($i = 1,\\ldots,n$) in the training set\n",
        "\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{CE}(\\mathbf{v}; \\mathbf{x}, \\mathbf{y}) = -\\prod_{i=1}^n \\log \\left(p(y_i \\vert \\mathbf{x}_i\\right) = - \\sum_{i = 1}^{n} \\left\\{ y_i \\log \\sigma(\\mathbf{w}^T \\mathbf{x}_i + b) + (1 - y_i) \\log (1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i + b)) \\right\\}\n",
        "$$\n",
        "\n",
        "Often an L2 regularisation term of the form $\\mathcal{L}_{\\mathrm{reg}}(\\mathbf{w}) = \\frac{1}{2} \\lambda \\mathbf{w}^T \\mathbf{w}$ is added to the objective to penalize large weights (this can help prevent overfitting to idiosycrancies in the training set). Note that $\\lambda \\geq 0$ controls the strength of the regularisation term.\n",
        "\n",
        "Putting this together, our goal is to minimise the following objective function with respect to $\\mathbf{v}$:\n",
        "$$\n",
        "\\mathcal{L}(\\mathbf{v}; \\mathbf{x}, \\mathbf{y}) = \\mathcal{L}_\\mathrm{reg}(\\mathbf{w}) + \\mathcal{L}_{CE}(\\mathbf{v}; \\mathbf{x}, \\mathbf{y})\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIRul5OuW5OQ",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "#### **Exercise 1 (Discussion?):** \n",
        "The L2 regularization term  $\\mathcal{L}_{\\mathrm{reg}}(\\mathbf{w}) = \\frac{1}{2} \\lambda \\mathbf{w}^T \\mathbf{w}$ is commonly said to reduce overfitting. Give a brief justification why?\n",
        "\n",
        "#### **Exercise 2 (Discussion?):** \n",
        "Why do we only include the weights $\\mathbf{w}$ in the L2 regularization term? i.e. the bias terms are excluded from regularization.\n",
        "\n",
        "#### **Exercise 3:**\n",
        "Given we model the conditional probability of label $y=1$ to be $p(y = 1 \\vert \\mathbf{x}) = \\sigma\\left(\\mathbf{w}^T \\mathbf{x} + b\\right)$, show that prediction is based on a linear decision rule given by the sign of logarithm of the ratio of probabilities:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\log \\frac{p(y=1 \\vert \\mathbf{x})}{p(y=0 \\vert \\mathbf{x})} = \\mathbf{w}^T \\mathbf{x} + b\n",
        "\\end{equation}\n",
        "\n",
        "This is why logistic regression is referred to as a _log-linear model_. What is the decision boundary for logistic regression? \n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a6Jz2SOW5OR",
        "colab_type": "text"
      },
      "source": [
        "We're going to find a solution to this minimisation problem using the BFGS algorithm (named after the inventors Broyden, Fletcher, Goldfarb and Shanno). BFGS is a \"hill-climbing\" algorithm like gradient descent, however it additionally makes use of second-order derivative information (by approximating the Hessian). It converges in fewer iterations than gradient descent (it's convergence rate is *superlinear* whereas gradient descent is only *linear*).\n",
        "\n",
        "We'll use an implementation of BFGS provided in `scipy` called `fmin_bfgs`. The algorithm requires two functions as input: (i) a function that evaluates the objective $f(\\mathbf{v}; \\ldots)$ and (ii) a function that evalutes the gradient $\\nabla_{\\mathbf{v}} f(\\mathbf{v}; \\ldots)$.\n",
        "\n",
        "Let's start by writing a function to compute $f(\\mathbf{v}; \\ldots)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6rEMujjW5OR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.special import expit # this is the logistic function\n",
        "sigmoid = expit\n",
        "\n",
        "# v: parameter vector\n",
        "# X: feature matrix\n",
        "# Y: class labels\n",
        "# Lambda: regularisation constant\n",
        "def obj_fn(v, X, Y, Lambda):\n",
        "    prob_1 = sigmoid(np.dot(X,v[1::]) + v[0])\n",
        "    reg_term = ... # fill in\n",
        "    cross_entropy_term = - np.dot(Y, np.log(prob_1)) - np.dot(1. - Y, np.log(1. - prob_1))\n",
        "    return ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iAK3HbZW5OT",
        "colab_type": "text"
      },
      "source": [
        "Now for the gradient, we use the following result (if you're familiar with vector calculus, you may wish to derive this yourself):\n",
        "$$\n",
        "\\nabla_{\\mathbf{v}} f(\\mathbf{v}; \\ldots) = \\left[\\frac{\\partial f(\\mathbf{w}, b;\\ldots)}{\\partial b}, \\nabla_{\\mathbf{w}} f(\\mathbf{w}, b; \\ldots) \\right] = \\left[\\sum_{i = 1}^{n} \\sigma(\\mathbf{w}^T \\mathbf{x}_i + b) - y_i, \\lambda \\mathbf{w} + \\sum_{i = 1}^{n} (\\sigma(\\mathbf{w}^T \\mathbf{x}_i + b) - y_i)\\mathbf{x}_i\\right]\n",
        "$$\n",
        "\n",
        "The function below implements $\\nabla_{\\mathbf{v}} f(\\mathbf{v}; \\ldots)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNqfhvooW5OT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# v: parameter vector\n",
        "# X: feature matrix\n",
        "# Y: class labels\n",
        "# Lambda: regularisation constant\n",
        "def grad_obj_fn(v, X, Y, Lambda):\n",
        "    prob_1 = sigmoid(np.dot(X, v[1::]) + v[0])\n",
        "    grad_b = np.sum(prob_1 - Y)\n",
        "    grad_w = Lambda * v[1::] + np.dot(prob_1 - Y, X)\n",
        "    return np.insert(grad_w, 0, grad_b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxebyDmpW5OV",
        "colab_type": "text"
      },
      "source": [
        "### 3. Solving the minimization problem using BFGS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "echZLV7LW5OV",
        "colab_type": "text"
      },
      "source": [
        "Now that we've implemented functions to compute the objective and the gradient, we can plug them into `fmin_bfgs`.\n",
        "Specifically, we define a function `my_logistic_regression` which calls `fmin_bfgs` and returns the optimal weight vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GemeTyl9W5OW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.optimize import fmin_bfgs\n",
        "\n",
        "# X: feature matrix\n",
        "# Y: class labels\n",
        "# Lambda: regularisation constant\n",
        "# v_initial: initial guess for parameter vector\n",
        "def my_logistic_regression(X, Y, Lambda, v_initial, disp=True):\n",
        "    # Function for displaying progress\n",
        "    def display(v):\n",
        "        print('v is', v, 'objective is', obj_fn(v, X, Y, Lambda))\n",
        "    \n",
        "    return fmin_bfgs(f=obj_fn, fprime=grad_obj_fn, \n",
        "                     x0=v_initial, args=(X, Y, Lambda), disp=disp, \n",
        "                     callback=display)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmJ0zx7hW5OY",
        "colab_type": "text"
      },
      "source": [
        "Let's try it out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NA6OYwTW5OY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Lambda = 1\n",
        "v_initial = np.zeros(X_train.shape[1] + 1) # fill in a vector of zeros of appropriate length\n",
        "# Hint: how many parameters in our model?\n",
        "v_opt = my_logistic_regression(X_train, Y_train, Lambda, v_initial)\n",
        "\n",
        "# Function to plot the data points and decision boundary\n",
        "def plot_results(X, Y, v, trans_func = None):\n",
        "    # Scatter plot in feature space\n",
        "    plt.plot(X[Y==0,0], X[Y==0,1], '.', label = \"$y=0$\", c='r')\n",
        "    plt.plot(X[Y==1,0], X[Y==1,1], '.', label = \"$y=1$\", c='g')\n",
        "    \n",
        "    # Compute axis limits\n",
        "    x0_lower = X[:,0].min() - 0.1\n",
        "    x0_upper = X[:,0].max() + 0.1\n",
        "    x1_lower = X[:,1].min() - 0.1\n",
        "    x1_upper = X[:,1].max() + 0.1\n",
        "    \n",
        "    # Generate grid over feature space\n",
        "    x0, x1 = np.mgrid[x0_lower:x0_upper:.01, x1_lower:x1_upper:.01]\n",
        "    grid = np.c_[x0.ravel(), x1.ravel()]\n",
        "    if (trans_func is not None):\n",
        "        grid = trans_func(grid) # apply transformation to features\n",
        "    arg = (np.dot(grid, v[1::]) + v[0]).reshape(x0.shape)\n",
        "    \n",
        "    # Plot decision boundary (where w^T x + b == 0)\n",
        "    plt.contour(x0, x1, arg, levels=[0], cmap=\"Greys\", vmin=-0.2, vmax=0.2)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "plot_results(X, Y, v_opt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oXQRag6W5Ob",
        "colab_type": "text"
      },
      "source": [
        "**Question:** Is the solution what you expected? Is it a good fit for the data?\n",
        "\n",
        "**Question:** What's the accuracy of this model? Fill in the code below assuming the following decision function\n",
        "$$\n",
        "\\hat{y} = \\begin{cases}\n",
        "    1, &\\mathrm{if} \\ p(y = 1|\\mathbf{x}) \\geq \\tfrac{1}{2}, \\\\\n",
        "    0, &\\mathrm{otherwise}.\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp8VuRYHW5Oc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def get_predictions(v_opt, X_test):\n",
        "    #  Implement the logistic regression prediction rule here\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    v_opt:        Optimized weights, shape [n+1]\n",
        "                  Should contain bias term as zeroth element\n",
        "    X_test:       Testing data, shape [N_test, n]\n",
        "    \n",
        "    Outputs:\n",
        "    Y_test_pred:  Vector of predictions for given test instances\n",
        "    \"\"\"\n",
        "    w = ...\n",
        "    b = ...\n",
        "    y_prob = ...\n",
        "    Y_test_pred = ... # Returns boolean array, which we cast to float\n",
        "    \n",
        "    return Y_test_pred\n",
        "\n",
        "Y_test_pred = get_predictions(v_opt, X_test)\n",
        "\n",
        "print('Accuracy achieved: {:.3f}'.format(accuracy_score(Y_test, Y_test_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt1v9wlTW5Of",
        "colab_type": "text"
      },
      "source": [
        "Writing nice docstrings allows your code to be self-documenting! :) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jL2sbEWW5Of",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_predictions?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSNxdNk2W5Oh",
        "colab_type": "text"
      },
      "source": [
        "### 4. Adding polynomial features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prJS5CwUW5Oh",
        "colab_type": "text"
      },
      "source": [
        "We've seen that ordinary logistic regression does poorly on this data set, because the data is not linearly separable in the $x_0,x_1$ feature space.\n",
        "\n",
        "We can get around this problem using basis expansion. In this case, we'll augment the feature space by adding polynomial features of degree 2. In other words, we replace the original feature matrix $\\mathbf{X}$ by a transformed feature matrix $\\mathbf{\\Phi}$ which contains additional columns corresponding to $x_0^2$, $x_0 x_1$ and $x_1^2$. This is done using the function `add_quadratic_features` defined below.\n",
        "\n",
        "**Note:** There's a built-in function in `sklearn` for adding polynomial features located at `sklearn.preprocessing.PolynomialFeatures`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPUB8wIDW5Oi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X: original feature matrix\n",
        "def add_quadratic_features(X):\n",
        "    return np.c_[X, X[:,0]**2, X[:,0]*X[:,1], X[:,1]**2]\n",
        "\n",
        "def add_cubic_features(X):\n",
        "    u, v= X[:,0], X[:,1]\n",
        "    return np.column_stack([X, u**2, u*v, v**2, u**2*v, u*v**2, u**3, v**3])\n",
        "\n",
        "Phi_train = add_quadratic_features(X_train)\n",
        "Phi_test = add_quadratic_features(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L55BdV4yW5Oj",
        "colab_type": "text"
      },
      "source": [
        "Lets check the shape of the augmented feature matrix $\\Phi$, is it what you expect?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbr7Mf0FW5Oj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(r'Shape of Phi:', Phi_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHDzgm71W5Ol",
        "colab_type": "text"
      },
      "source": [
        "Let's apply our custom logistic regression function again on the augmented feature space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igdl2n1uW5Ol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Lambda = 1\n",
        "v_initial = np.zeros(Phi_train.shape[1] + 1) # fill in a vector of zeros of appropriate length\n",
        "v_opt_phi = my_logistic_regression(Phi_train, Y_train, Lambda, v_initial)\n",
        "plot_results(X, Y, v_opt_phi, trans_func=add_quadratic_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B3PzYJgW5On",
        "colab_type": "text"
      },
      "source": [
        "This time we should get a better result for the accuracy on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPL9ZcSBW5On",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "Y_test_pred = ...  # fill in\n",
        "print('Accuracy achieved: {:.3f}'.format(accuracy_score(Y_test, Y_test_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo_uKA9rW5Op",
        "colab_type": "text"
      },
      "source": [
        "### 5. Effect of regularisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StnI2y6cW5Op",
        "colab_type": "text"
      },
      "source": [
        "So far, we've fixed the regularisation constant so that $\\lambda = 1$. (Note it's possible to choose an \"optimal\" value for $\\lambda$ by applying cross-validation.)\n",
        "\n",
        "**Question:** What do you think will happen if we switch the regularisation off? Try setting $\\lambda$ to a small value (say $10^{-3}$) and check whether the accuracy of the model is affected. You may wish to scan across a range of values for $\\lambda$ and observe which value gives you the best accuracy on the test set. You may want to experiment with cubic or another monomial features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQSZxnoQW5Oq",
        "colab_type": "text"
      },
      "source": [
        "### 6. Logistic regression using sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5YIIMbBW5Or",
        "colab_type": "text"
      },
      "source": [
        "Now that you have some insight into the optimisation problem behind logistic regression, you should feel confident in using the built-in implementation in `sklearn` (or other packages).\n",
        "Note that the `sklearn` implementation handles floating point underflow/overflow more carefully than we have done, and uses faster numerical optimisation algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOLN5iNfW5Os",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(C=1)\n",
        "clf.fit(Phi_train, Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ5rMabvW5Ow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "Y_test_pred = clf.predict(Phi_test)\n",
        "accuracy_score(Y_test, Y_test_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAuLlCp9W5Oy",
        "colab_type": "text"
      },
      "source": [
        "### Bonus\n",
        "#### **Exercise 4**\n",
        "Consider how you would extend logistic regression to the multiclass case where $y \\in \\{c_1, \\ldots c_m\\}$. How would you model the conditional probability for class $k$: $p(y=c_k \\vert \\mathbf{x})$?"
      ]
    }
  ]
}