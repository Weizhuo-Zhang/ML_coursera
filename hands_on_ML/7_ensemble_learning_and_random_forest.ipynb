{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Weizhuo-Zhang/ML_coursera/blob/master/hands_on_ML/7_ensemble_learning_and_random_forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttPPRw6HSoqJ",
        "colab_type": "text"
      },
      "source": [
        "# 七、集成学习和随机森林 (Ensemble Learning and Ransom Forest)\n",
        "\n",
        "## 投票分类\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URaBDfDPSQba",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "0f92336f-188f-4635-be81-da6db3413a69"
      },
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = make_moons(noise=0.1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "log_clf = LogisticRegression()\n",
        "rnd_clf = RandomForestClassifier()\n",
        "svm_clf = SVC()\n",
        "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf),\n",
        "    ('svc', svm_clf)], voting='hard')\n",
        "voting_clf.fit(X_train, y_train);"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiG6WkJFHeIp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "9a859140-0c97-48c1-ad82-c76ea4a9fa54"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
        "  clf.fit(X_train, y_train)\n",
        "  y_pred = clf.predict(X_test)\n",
        "  print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression 0.85\n",
            "RandomForestClassifier 0.95\n",
            "SVC 0.9\n",
            "VotingClassifier 0.9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnxPHAkAIQw6",
        "colab_type": "text"
      },
      "source": [
        "## Bagging 和 Pasting\n",
        "\n",
        "就像之前讲到的，可以通过使用不同的训练算法去得到一些不同的分类器。另一种方法就是对每一个分类器都使用相同的训练算法，但是在不同的训练集上去训练它们。有放回采样被称为装袋（`Bagging`，是 `bootstrap aggregating` 的缩写）。无放回采样称为粘贴（`pasting`）。\n",
        "\n",
        "换句话说，`Bagging` 和 `Pasting` 都允许在多个分类器上对训练集进行多次采样，但只有 `Bagging` 允许对同一种分类器上对训练集进行进行多次采样。采样和训练过程如图7-4所示。\n",
        "\n",
        "### Sklearn 中的 Bagging 和 Pasting\n",
        "\n",
        "`Sklearn` 为 `Bagging` 和 `Pasting` 提供了一个简单的API： `BaggingClassifier` 类（或者对于回归可以是 `BaggingRegressor` 。接下来的代码训练了一个 500 个决策树分类器的集成，每一个都是在数据集上有放回采样 100 个训练实例下进行训练（这是 `Bagging` 的例子，如果你想尝试 `Pasting`，就设置 `bootstrap=False` ）。 `n_jobs` 参数告诉 `Sklearn` 用于训练和预测所需要 `CPU`核的数量。（-1 代表着 sklearn 会使用所有空闲核）："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5PK-tiJH4PS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
        "    max_samples=80, bootstrap=True, n_jobs=-1)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred = bag_clf.predict(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5t8FyWIKH-c",
        "colab_type": "text"
      },
      "source": [
        "如果基分类器可以预测类别概率（例如它拥有 `predict_proba()` 方法），那么 `BaggingClassifier` 会自动的运行软投票，这是决策树分类器的情况。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elM0GPW3Kbok",
        "colab_type": "text"
      },
      "source": [
        "### Out-of-Bag 评价\n",
        "\n",
        "对于 `Bagging` 来说，一些实例可能被一些分类器重复采样，但其他的有可能不会被采样。 `BaggingClassifier` 默认采样。 `BaggingClassifier` 默认是有放回的采样 m 个实例（ `bootstrap=True` ），其中 m 是训练集的大小，这意味着平均下来只有63%的训练实例被每个分类器采样，剩下的37%个没有被采样的训练实例就叫做 `Out-of-Bag` 实例。注意对于每一\n",
        "个的分类器它们的 37% 不是相同的。\n",
        "\n",
        "因为在训练中分类器从来没有看到过 `Out-of-Bag` 实例，所以它可以在这些实例上进行评估，而不需要单独的验证集或交叉验证。你可以拿出每一个分类器的 oob 来评估集成本身。\n",
        "\n",
        "在 `sklearn` 中，你可以在训练后需要创建一个 `BaggingClassifier` 来自动评估时设置 `oob_score=True` 来自动评估。接下来的代码展示了这个操作。评估结果通过变\n",
        "量 `oob_score_` 来显示："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV356IbLJ4Fy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b987c4c-c73d-4cc9-f80e-8d9e0198bd62"
      },
      "source": [
        "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, bootstrap=True,\n",
        "    n_jobs=-1, oob_score=True)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "bag_clf.oob_score_"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.925"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTTyqRICLpak",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "83d20335-f5c0-4c60-f111-1550b3472c96"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.95"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sezmPO2mMJ4F",
        "colab_type": "text"
      },
      "source": [
        "对于每个训练实例 oob 决策函数也可通过 `oob_decision_function_` 变量来展示。在这种情况下（当基决策器有 `predict_proba()` 时）决策函数会对每个训练实例返回类别概率。例如，oob 评估预测第二个训练实例有 60.6% 的概率属于正类（39.4% 属于负类）："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jILn0fMMS_U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6f5ceb23-6d5a-44da-8c42-7d3ba1ba3035"
      },
      "source": [
        "bag_clf.oob_decision_function_"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.02538071, 0.97461929],\n",
              "       [0.16176471, 0.83823529],\n",
              "       [0.24731183, 0.75268817],\n",
              "       [0.01052632, 0.98947368],\n",
              "       [0.        , 1.        ],\n",
              "       [0.12234043, 0.87765957],\n",
              "       [0.97222222, 0.02777778],\n",
              "       [0.        , 1.        ],\n",
              "       [0.59649123, 0.40350877],\n",
              "       [0.99453552, 0.00546448],\n",
              "       [0.97382199, 0.02617801],\n",
              "       [0.84803922, 0.15196078],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.01621622, 0.98378378],\n",
              "       [0.93717277, 0.06282723],\n",
              "       [1.        , 0.        ],\n",
              "       [0.73469388, 0.26530612],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.05780347, 0.94219653],\n",
              "       [1.        , 0.        ],\n",
              "       [0.29281768, 0.70718232],\n",
              "       [0.14659686, 0.85340314],\n",
              "       [0.24858757, 0.75141243],\n",
              "       [0.04891304, 0.95108696],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.01595745, 0.98404255],\n",
              "       [0.        , 1.        ],\n",
              "       [0.77653631, 0.22346369],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.07222222, 0.92777778],\n",
              "       [0.8579235 , 0.1420765 ],\n",
              "       [0.4125    , 0.5875    ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.11340206, 0.88659794],\n",
              "       [0.        , 1.        ],\n",
              "       [0.01724138, 0.98275862],\n",
              "       [0.86857143, 0.13142857],\n",
              "       [0.75274725, 0.24725275],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.94736842, 0.05263158],\n",
              "       [0.        , 1.        ],\n",
              "       [0.99418605, 0.00581395],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.00555556, 0.99444444],\n",
              "       [0.93908629, 0.06091371],\n",
              "       [0.93717277, 0.06282723],\n",
              "       [1.        , 0.        ],\n",
              "       [0.99431818, 0.00568182],\n",
              "       [1.        , 0.        ],\n",
              "       [0.10810811, 0.89189189],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.5920398 , 0.4079602 ],\n",
              "       [0.17204301, 0.82795699],\n",
              "       [0.01818182, 0.98181818],\n",
              "       [0.93582888, 0.06417112],\n",
              "       [0.0625    , 0.9375    ],\n",
              "       [0.98742138, 0.01257862],\n",
              "       [0.        , 1.        ],\n",
              "       [0.16571429, 0.83428571],\n",
              "       [0.93333333, 0.06666667]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U35U08pgMgb2",
        "colab_type": "text"
      },
      "source": [
        "## 随机贴片与随机子空间\n",
        "\n",
        "`BaggingClassifier` 也支持采样特征。它被两个超参数 `max_features` 和 `bootstrap_features` 控制。他们的工作方式和 `max_samples` 和 `bootstrap` 一样，但这是对于特征采样而不是实例采样。因此，每一个分类器都会被在随机的输入特征内进行训练。\n",
        "\n",
        "当你在处理高维度输入下（例如图片）此方法尤其有效。对训练实例和特征的采样被叫做随机贴片。保留了所有的训练实例（例如 `bootstrap=False` 和 `max_samples=1.0` ），但是对特征采样（ `bootstrap_features=True` 并且/或者 `max_features` 小于 1.0）叫做随机子空间。采样特征导致更多的预测多样性，用高偏差换低方差。\n",
        "\n",
        "## 随机森林\n",
        "\n",
        "正如我们所讨论的，随机森林是决策树的一种集成，通常是通过 `bagging` 方法（有时是 `pasting` 方法）进行训练，通常用 `max_samples` 设置为训练集的大小。与建立一个 `BaggingClassifier` 然后把它放入 `DecisionTreeClassifier` 相反，你可以使用更方便的也是对决策树优化够的 `RandomForestClassifier` （对于回归是 `RandomForestRegressor` ）。接下来\n",
        "的代码训练了带有 500 个树（每个被限制为 16 叶子结点）的决策森林，使用所有空闲的CPU 核："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDqX6aJoMV0M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
        "rnd_clf.fit(X_train, y_train)\n",
        "y_pred_rf = rnd_clf.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8Ev4A0XPQ5H",
        "colab_type": "text"
      },
      "source": [
        "除了一些例外， `RandomForestClassifier` 使用 `DecisionTreeClassifier` 的所有超参数（决定数怎么生长），把 `BaggingClassifier` 的超参数加起来来控制集成本身。\n",
        "\n",
        "\n",
        "随机森林算法在树生长时引入了额外的随机；与在节点分裂时需要找到最好分裂特征相反（详见第六章），它在一个随机的特征集中找最好的特征。它导致了树的差异性，并且再一次用高偏差换低方差，总的来说是一个更好的模型。以下是 `BaggingClassifier` 大致相当于之前的 `randomForestClassifier` ："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqJ4iJ7ZPQG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bag_clf = BaggingClassifier(DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
        "    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M2KrtYsQS1_",
        "colab_type": "text"
      },
      "source": [
        "### 极端随机树\n",
        "\n",
        "### 特征重要度\n",
        "\n",
        "最后，如果你观察一个单一决策树，重要的特征会出现在更靠近根部的位置，而不重要的特征会经常出现在靠近叶子的位置。因此我们可以通过计算一个特征在森林的全部树中出现的平均深度来预测特征的重要性。`Sklearn` 在训练后会自动计算每个特征的重要度。你可以通过 `feature_importances_` 变量来查看结果。例如如下代码在 iris 数据集（第四章介绍）上训练了一个 `RandomForestClassifier` 模型，然后输出了每个特征的重要性。看来，最重要的特征是花瓣长度（44%）和宽度（42%），而萼片长度和宽度相对比较是不重要的（分别为 11% 和 2%）：\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFxeD7RdPzTO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "041e0ed9-3cde-4902-cef6-f052501a3864"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
        "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
        "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
        "  print(name, score)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sepal length (cm) 0.09398536614729343\n",
            "sepal width (cm) 0.025942194025263703\n",
            "petal length (cm) 0.449178296090451\n",
            "petal width (cm) 0.4308941437369918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZYJeBE8R0IQ",
        "colab_type": "text"
      },
      "source": [
        "## 提升 (Boosting)\n",
        "\n",
        "提升（`Boosting`，最初称为假设增强）指的是可以将几个弱学习者组合成强学习者的集成方法。对于大多数的提升方法的思想就是按顺序去训练分类器，每一个都要尝试修正前面的分类。现如今已经有很多的提升方法了，但最著名的就是 `Adaboost`（适应性提升，是 `Adaptive Boosting` 的简称） 和 `Gradient Boosting`（梯度提升）。\n",
        "\n",
        "### Adaboost\n",
        "\n",
        "使一个新的分类器去修正之前分类结果的方法就是对之前分类结果不对的训练实例多加关注。这导致新的预测因子越来越多地聚焦于这种情况。这是 Adaboost 使用的技术。\n",
        "\n",
        "举个例子，去构建一个 Adaboost 分类器，第一个基分类器（例如一个决策树）被训练然后在训练集上做预测，在误分类训练实例上的权重就增加了。第二个分类机使用更新过的权重然后再一次训练，权重更新，以此类推（详见图 7-7）\n",
        "\n",
        "sklearn 通常使用 Adaboost 的多分类版本 SAMME（这就代表了 分段加建模使用多类指数损失函数）。如果只有两类别，那么 SAMME 是与 Adaboost 相同的。如果分类器可以预测类别概率（例如如果它们有 predict_proba() ），如果 sklearn 可以使用 SAMME 叫做 SAMME.R 的变量（R 代表“REAL”），这种依赖于类别概率的通常比依赖于分类器的更好。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NEhYaeGXItC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "79a795f4-9ece-4cb5-b460-21d36875761b"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n",
        "    n_estimators=200, algorithm=\"SAMME.R\", learning_rate=0.5)\n",
        "print(ada_clf.fit(X_train, y_train))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AdaBoostClassifier(algorithm='SAMME.R',\n",
            "                   base_estimator=DecisionTreeClassifier(class_weight=None,\n",
            "                                                         criterion='gini',\n",
            "                                                         max_depth=1,\n",
            "                                                         max_features=None,\n",
            "                                                         max_leaf_nodes=None,\n",
            "                                                         min_impurity_decrease=0.0,\n",
            "                                                         min_impurity_split=None,\n",
            "                                                         min_samples_leaf=1,\n",
            "                                                         min_samples_split=2,\n",
            "                                                         min_weight_fraction_leaf=0.0,\n",
            "                                                         presort=False,\n",
            "                                                         random_state=None,\n",
            "                                                         splitter='best'),\n",
            "                   learning_rate=0.5, n_estimators=200, random_state=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gFquTWbY0R-",
        "colab_type": "text"
      },
      "source": [
        "### Gradient Boost 梯度提升"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z85CXsmwXgvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg1.fit(X, y)\n",
        "\n",
        "# 现在在第一个分类器的残差上训练第二个分类器\n",
        "y2 = y_train - tree_reg1.predict(X_train)\n",
        "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg2.fit(X_train, y2)\n",
        "\n",
        "# 现在在第二个分类器的残差上训练第三个分类器\n",
        "y3 = y2 - tree_reg2.predict(X_train)\n",
        "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg3.fit(X_train, y3)\n",
        "\n",
        "y_pred = sum(tree.predict(X_test) for tree in (tree_reg1, tree_reg2, tree_reg3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvpQS8T_aC8G",
        "colab_type": "text"
      },
      "source": [
        "我们可以使用 sklean 中的 `GradientBoostingRegressor` 来训练 GBRT (Gradient Tree Boosting or Gradient Boosted Regression Tree) 集成。与 `RandomForestClassifier` 相似，它也有超参数去控制决策树的生长（例如 `max_depth` ， `min_samples_leaf` 等等），也有超参数去控制集成训练，例如基分类器的数量（ `n_estimators` ）。接下来的代码创建了与之前相同的集成："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0R942VRZtnJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "cca78744-7869-4645-f742-011d70c579f1"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
        "gbrt.fit(X, y)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
              "                          learning_rate=1.0, loss='ls', max_depth=2,\n",
              "                          max_features=None, max_leaf_nodes=None,\n",
              "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                          min_samples_leaf=1, min_samples_split=2,\n",
              "                          min_weight_fraction_leaf=0.0, n_estimators=3,\n",
              "                          n_iter_no_change=None, presort='auto',\n",
              "                          random_state=None, subsample=1.0, tol=0.0001,\n",
              "                          validation_fraction=0.1, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6glGjMv_aqXS",
        "colab_type": "text"
      },
      "source": [
        "超参数 `learning_rate` 确立了每个树的贡献。如果你把它设置为一个很小的树，例如 0.1，在集成中就需要更多的树去拟合训练集，但预测通常会更好。这个正则化技术叫做 `shrinkage`。\n",
        "\n",
        "为了找到树的最优数量，你可以使用早停技术（第四章讨论）。最简单使用这个技术的方法就是使用 `staged_predict()`` ：它在训练的每个阶段（用一棵树，两棵树等）返回一个迭代器。加下来的代码用 120 个树训练了一个 GBRT 集成，然后在训练的每个阶段验证错误以找到树的最佳数量，最后使用 GBRT 树的最优数量训练另一个集成："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7qoFrCLa-K8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "84193a02-fcd8-41e0-c185-90e1306f1efc"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
        "gbrt.fit(X_train, y_train)\n",
        "errors = [mean_squared_error(y_val, y_pred)\n",
        "    for y_pred in gbrt.staged_predict(X_val)]\n",
        "bst_n_estimators = np.argmin(errors)\n",
        "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators)\n",
        "gbrt_best.fit(X_train, y_train)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
              "                          learning_rate=0.1, loss='ls', max_depth=2,\n",
              "                          max_features=None, max_leaf_nodes=None,\n",
              "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                          min_samples_leaf=1, min_samples_split=2,\n",
              "                          min_weight_fraction_leaf=0.0, n_estimators=85,\n",
              "                          n_iter_no_change=None, presort='auto',\n",
              "                          random_state=None, subsample=1.0, tol=0.0001,\n",
              "                          validation_fraction=0.1, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4j9TPDJbvLa",
        "colab_type": "text"
      },
      "source": [
        "你可以通过设置 `warm_start=True` 来实现 ，这使得当 `fit()` 方法被调用时 `sklearn` 保留现有树，并允许增量训练。接下来的代码在当一行中的五次迭代验证错误没有改善时会停止\n",
        "训练："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-8u_8Oeb2oz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
        "min_val_error = float(\"inf\")\n",
        "error_going_up = 0\n",
        "for n_estimators in range(1, 120):\n",
        "  gbrt.n_estimators = n_estimators\n",
        "  gbrt.fit(X_train, y_train)\n",
        "  y_pred = gbrt.predict(X_val)\n",
        "  val_error = mean_squared_error(y_val, y_pred)\n",
        "  if val_error < min_val_error:\n",
        "    min_val_error = val_error\n",
        "    error_going_up = 0\n",
        "  else:\n",
        "    error_going_up += 1\n",
        "    if error_going_up == 5:\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwaTU01fchCj",
        "colab_type": "text"
      },
      "source": [
        "## Stacking\n",
        "\n",
        "**Stacking** (Stacked generalization) \n",
        "\n",
        "这个算法基于一个简单的想法：不使用琐碎的函数（如硬投票）来聚合集合中所有分类器的预测，我们为什么不训练一个模型来执行这个聚合？图 7-12 展示了这样一个在新的回归实例上预测的集\n",
        "成。底部三个分类器每一个都有不同的值（3.1，2.7 和 2.9），然后最后一个分类器（叫做 `blender` 或者 `meta learner` ）把这三个分类器的结果当做输入然后做出最终决策（3.0）。\n",
        "\n",
        "![stackingGenerization](https://raw.githubusercontent.com/Weizhuo-Zhang/ML_coursera/master/hands_on_ML/pics/stackingGenerization.jpg)"
      ]
    }
  ]
}