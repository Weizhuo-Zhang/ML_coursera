{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Weizhuo-Zhang/ML_coursera/blob/master/hands_on_ML/8_dimensionality_reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Pklm2obkjgK",
        "colab_type": "text"
      },
      "source": [
        "# 八、降维\n",
        "\n",
        "很多机器学习的问题都会涉及到几千甚至数百万维的特征的训练实例。这叫做维数灾难(Curse of dimentionality)。\n",
        "\n",
        "**警告：**\n",
        "\n",
        "降维肯定会丢失一些信息（这就好比将一个图片压缩成JPEG的格式会降低图片的质量）\n",
        "\n",
        "## 降维的主要方法\n",
        "\n",
        "## 投影(Projection)\n",
        "\n",
        "## 流行学习(Manifold learning)\n",
        "\n",
        "## 主成分分析(PCA) Principal Component Analysis\n",
        "\n",
        "### 保留(最大)方差\n",
        "\n",
        "### 主成分(Principal Components)\n",
        "\n",
        "奇异值分解(SVD)的标准矩阵分解技术，可以找到训练集的主成分。\n",
        "\n",
        "下面的Python代码使用了Numpy提供的`svd()`函数获得训练集的所有主成分，然后提取前两个PC：\n",
        "\n",
        "```python\n",
        "X_centered = X - X.mean(axis=0)\n",
        "U, s, V = np.linalg.svd(X_centered)\n",
        "c1 = V.T[:,0]\n",
        "c2 = V.T[:,1]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S5UCNw0oca7",
        "colab_type": "text"
      },
      "source": [
        "**警告：**\n",
        "\n",
        "PCA假定数据集以原点为中心。如果自己实现PCA，不要忘记首先要对数据做中心化处理。\n",
        "\n",
        "### 投影到 d 维空间\n",
        "\n",
        "一旦确认了所有的主成分，就可以将数据集投影到由前 `d` 个成分构成的超平面上，从而将数据集的维数降低至 `d` 维，选择的这个超平面可以确保留尽可能多的方差。\n",
        "\n",
        "**公式：** 将训练集投影到 `d` 维空间：\n",
        "$$X_{d-proj} =  X\\cdot W_d$$\n",
        "\n",
        "\n",
        "**代码**\n",
        "```python\n",
        "W2 = V.T[:, :2]\n",
        "X2D = X_centered.dot(W2)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsoKcWkKsJft",
        "colab_type": "text"
      },
      "source": [
        "### 使用 Scikit-Learn\n",
        "\n",
        "**`Scikit-Learn`** 的 PCA 类使用 SVD 分解来实现。以下代码应用 PCA 将数据集的维度降低至两维\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X2D=pca.fit_transform(X)\n",
        "```\n",
        "\n",
        "PCA，可以使用 `components_` 访问每一个主成分（注意，它返回以 PC 作为水平向量的矩阵，因此，如果我们想要获得第一个主成分则可以写成 `pca.components_.T[:,0]` ）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlwsBEADt2zy",
        "colab_type": "text"
      },
      "source": [
        "### 方差解释率(Explained Variance Ratio)\n",
        "\n",
        "另一个非常有用的信息是每个主成分的方差解释率，可以通过 `explained_variance_ratio_` 变量获得。它表示位于每个主成分轴上的数据集方差的比例。\n",
        "\n",
        "```python\n",
        ">>> print(pca.explained_variance_ratio_)\n",
        "array([0.845433, 0.143422])\n",
        "```\n",
        "\n",
        "这表明，84.5%的数据集方差位于第一轴，14.3%的方差位于第二轴。第三周比例小，可能没有包含什么信息。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CqNZtiVutxl",
        "colab_type": "text"
      },
      "source": [
        "### 选择正确的维度\n",
        "\n",
        "下面的代码在不降维的情况下进行 PCA，然后计算出保留训练集方差 95% 所需的最小维数：\n",
        "\n",
        "```python\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
        "d = np.argmax(cumsum >= 0.95) + 1\n",
        "```\n",
        "\n",
        "可以设置 `n_components = d` 再次运行 PCA。但是，有一个更好的选择：不指定想要保留的主成分个数，而是将 `n_components` 设置为0.0 到1.0 之间的浮点数，表明您希望保留的方差比率：\n",
        "```python\n",
        "pca = PCA(n_components=0.95)\n",
        "X_reduced = pca.fit_transform(X)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8QSsMxdz199",
        "colab_type": "text"
      },
      "source": [
        "PCA 压缩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gK3NBEFoZKp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}