<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  Copyright (c) 2016-2018 Qualcomm Technologies, Inc.
  All Rights Reserved.
  Confidential and Proprietary - Qualcomm Technologies, Inc.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
-->
<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"></meta>
<meta http-equiv="X-UA-Compatible" content="IE=9"></meta>
<title>Snapdragon Neural Processing Engine SDK: Tools</title>
<link href="tabs.css" rel="stylesheet" type="text/css"></link>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="autoEnterCurrentDate.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="is.css" rel="stylesheet" type="text/css" ></link>
<link href="custom.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Snapdragon Neural Processing Engine SDK
   <span id="projectnumber"></span></div>
   <div id="projectbrief">Reference Guide</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('tools.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Tools </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This chapter describes the various SDK tools and features. </p><ul>
<li>
<a class="el" href="tools.html#tools_snpe-net-run">snpe-net-run</a>  </li>
<li>
<a class="el" href="tools.html#tools_snpe-bench">snpe_bench.py</a>  </li>
<li>
<a class="el" href="tools.html#tools_snpe-caffe-to-dlc">snpe-caffe-to-dlc</a>  </li>
<li>
<a class="el" href="tools.html#tools_snpe-caffe2-to-dlc">snpe-caffe2-to-dlc</a>  </li>
<li>
<a class="el" href="tools.html#tools_snpe-diagview">snpe-diagview</a>  </li>
<li>
<a class="el" href="tools.html#tools_snpe-dlc-info">snpe-dlc-info</a>  </li>
<li>
<a class="el" href="tools.html#tools_snpe-dlc-diff">snpe-dlc-diff</a>   </li>
<li>
<a class="el" href="tools.html#tools_snpe-dlc-viewer">snpe-dlc-viewer</a>  </li>
<li>
<a class="el" href="tools.html#tools_snpe-dlc-quantize">snpe-dlc-quantize</a>  </li>
<li>
<a class="el" href="tools.html#tools_snpe-tensorflow-to-dlc">snpe-tensorflow-to-dlc</a>  </li>
<li>
<a class="el" href="tools.html#tools_snpe-onnx-to-dlc">snpe-onnx-to-dlc</a>  </li>
<li>
<a class="el" href="tools.html#tools_snpe-platform-validator">snpe-platform-validator</a>  </li>
<li>
<a class="el" href="tools.html#tools_snpe-platform-validator-py">snpe-platform-validator-py</a>  </li>
<li>
<a class="el" href="tools.html#tools_snpe-throughput-net-run">snpe-throughput-net-run</a>  </li>
</ul>
<hr/>
 <h1><a class="anchor" id="tools_snpe-net-run"></a>
snpe-net-run</h1>
<p>snpe-net-run loads a DLC file, loads the data for the input tensor(s), and executes the network on the specified runtime.</p>
<pre class="fragment">DESCRIPTION:
------------
Example application demonstrating how to load and execute a neural network
using the SNPE C++ API.


REQUIRED ARGUMENTS:
-------------------
  --container  &lt;FILE&gt;   Path to the DL container containing the network.
  --input_list &lt;FILE&gt;   Path to a file listing the inputs for the network.


OPTIONAL ARGUMENTS:
-------------------
  --use_fxp_cpu         Use the CPU fixed point runtime for SNPE.
  --use_gpu             Use the GPU runtime for SNPE.
  --use_dsp             Use the DSP fixed point runtime for SNPE.
  --use_aip             Use the AIP fixed point runtime for SNPE.
  --debug               Specifies that output from all layers of the network
                        will be saved.
  --output_dir &lt;DIR&gt;    The directory to save output to. Defaults to ./output
  --storage_dir &lt;DIR&gt;   The directory to store SNPE metadata files
  --encoding_type &lt;VAL&gt; Specifies the encoding type of input file. Valid settings are "nv21".
                        Cannot be combined with --userbuffer*.
  --userbuffer_float    [EXPERIMENTAL] Specifies to use userbuffer for inference, and the input type is float.
                        Cannot be combined with --encoding_type.
  --userbuffer_tf8      [EXPERIMENTAL] Specifies to use userbuffer for inference, and the input type is tf8exact0.
                        Cannot be combined with --encoding_type.
  --perf_profile &lt;VAL&gt;  Specifies perf profile to set. Valid settings are "system_settings" , "power_saver" , "balanced" ,
                        "default" , "high_performance" , "sustained_high_performance" , and "burst".
                        NOTE: "balanced" and "default" are the same.  "default" is being deprecated in the future.
  --profiling_level &lt;VAL&gt; Specifies the profiling level.  Valid settings are "off", "basic" and "detailed".
                          Default is detailed.
  --enable_cpu_fallback Enables cpu fallback functionality. Defaults to disable mode.
  --input_name &lt;INPUT_NAME&gt; Specifies the name of input for which dimensions are specified.

  --input_dimensions &lt;INPUT_DIM&gt;  Specifies new dimensions for input whose name is specified in input_name. e.g. "1,224,224,3".
                        For multiple inputs, specify --input_name and --input_dimensions multiple times.
  --gpu_mode &lt;VAL&gt;      Specifies gpu operation mode. Valid settings are "default", "float16".
                        default = float32 math and float16 storage (equiv. use_gpu arg).
                        float16 = float16 math and float16 storage.
  --help                 Show this help message.
  --version              Show SNPE Version Number.</pre><p>This binary outputs raw output tensors into the output folder by default. Examples of using snpe-net-run can be found in <a class="el" href="tutorial_alexnet.html">Running AlexNet</a> tutorial.<br />
 <br />
 Additional details:<br />
 </p><ul>
<li>
<em>Running batched inputs:</em><a class="anchor" id="tools_snpe-net-run_batched_input"></a> <ul>
<li>
<p class="startli">snpe-net-run is able to automatically batch the input data. The batch size is indicated in the model container (DLC file) but can also be set using the "input_dimensions" argument passed to snpe-net-run. Users do not need to batch their input data. If the input data is not batch, the input size needs to be a multiple of the size of the input data files. snpe-net-run would group the provided inputs into batches and pad the incomplete batches (if present) with zeros.</p>
<p class="endli">In the example below, the model is set to accept batches of three inputs. So, the inputs are automatically grouped together to form batches by snpe-net-run and padding is done to the final batch. Note that there are five output files generated by snpe-net-run: </p><pre class="fragment">      …
      Processing DNN input(s):
      cropped/notice_sign.raw
      cropped/trash_bin.raw
      cropped/plastic_cup.raw
      Processing DNN input(s):
      cropped/handicap_sign.raw
      cropped/chairs.raw
      Applying padding</pre>  </li>
</ul>
</li>
<li>
<em>input_list argument:</em><a class="anchor" id="tools_snpe-net-run_input_list"></a> <ul>
<li>
<p class="startli">snpe-net-run can take multiple input files as input data per iteration, and specify multiple output names, in an input list file formated as below: </p><pre class="fragment">      #&lt;output_name&gt;[&lt;space&gt;&lt;output_name&gt;]
      &lt;input_layer_name&gt;:=&lt;input_layer_path&gt;[&lt;space&gt;&lt;input_layer_name&gt;:=&lt;input_layer_path&gt;]
      …</pre><p> The first line starting with a "#" specifies the output layers' names. If there is more than one output, a whitespace should be used as a delimiter. Following the first line, you can use multiple lines to supply input files, one line per iteration, and each line only supply one layer.If there is more than one input per line, a whitespace should be used as a delimiter.</p>
<p class="endli">Here is an example, where the layer names are "Input_1" and "Input_2", and inputs are located in the path "Placeholder_1/real_input_inputs_1/". Its input list file should look like this: </p><pre class="fragment">      #Output_1 Output_2
      Input_1:=Placeholder_1/real_input_inputs_1/0-0#e6fb51.rawtensor Input_2:=Placeholder_1/real_input_inputs_1/0-1#8a171b.rawtensor
      Input_1:=Placeholder_1/real_input_inputs_1/1-0#67c965.rawtensor Input_2:=Placeholder_1/real_input_inputs_1/1-1#54f1ff.rawtensor
      Input_1:=Placeholder_1/real_input_inputs_1/2-0#b42dc6.rawtensor Input_2:=Placeholder_1/real_input_inputs_1/2-1#346a0e.rawtensor</pre><p> <b>Note:</b> If the batch dimension of the model is greater than 1, the number of batch elements in the input file has to either match the batch dimension specified in the DLC or it has to be one. In the latter case, snpe-net-run will combine multiple lines into a single input tensor.  </p>
</li>
</ul>
</li>
</ul>
<ul>
<li>
<em>Running CPU Fixed Point Runtime:</em><a class="anchor" id="tools_snpe-net-run_fxp_cpu"></a> <ul>
<li>
The CPU Fixed Point Runtime requires a quantized DLC and cannot convert a non-quantized DLC automatically. The quantization parameters in the DLC will be used for each output layer unless the layer is constrained to use the same input and output quantization parameters for speed and accuracy.  </li>
</ul>
</li>
</ul>
<ul>
<li>
<em>Running AIP Runtime:</em><a class="anchor" id="tools_snpe-net-run_aip"></a> <ul>
<li>
AIP Runtime requires a DLC which was quantized, and HTA sections were generated offline. See <a class="el" href="hta_support.html">Adding HTA sections</a>  </li>
<li>
AIP Runtime does not support debug_mode  </li>
<li>
AIP Runtime does not support batch  </li>
</ul>
</li>
</ul>
<hr/>
 <h1><a class="anchor" id="tools_snpe-bench"></a>
snpe_bench.py</h1>
<p>python script snpe_bench.py runs a DLC neural network and collects benchmark performance information.</p>
<pre class="fragment">usage: snpe_bench.py [-h] -c CONFIG_FILE [-o OUTPUT_BASE_DIR_OVERRIDE]
                     [-v DEVICE_ID_OVERRIDE] [-r HOST_NAME] [-a]
                     [-t DEVICE_OS_TYPE_OVERRIDE] [-d] [-s SLEEP]
                     [-b USERBUFFER_MODE] [-p PERFPROFILE] [-l PROFILINGLEVEL]
                     [-json] [-cache]

Run the snpe_bench

required arguments:
  -c CONFIG_FILE, --config_file CONFIG_FILE
                        Path to a valid config file
                        Refer to sample config file config_help.json for more
                        detail on how to fill params in config file

optional arguments:
  -o OUTPUT_BASE_DIR_OVERRIDE, --output_base_dir_override OUTPUT_BASE_DIR_OVERRIDE
                        Sets the output base directory.
  -v DEVICE_ID_OVERRIDE, --device_id_override DEVICE_ID_OVERRIDE
                        Use this device ID instead of the one supplied in config
                        file. Cannot be used with -a
  -r HOST_NAME, --host_name HOST_NAME
                        Hostname/IP of remote machine to which devices are
                        connected.
  -a, --run_on_all_connected_devices_override
                        Runs on all connected devices, currently only support 1.
                        Cannot be used with -v
  -t DEVICE_OS_TYPE_OVERRIDE, --device_os_type_override DEVICE_OS_TYPE_OVERRIDE
                        Specify the target OS type, valid options are
                        ['android', 'android-aarch64', 'le', 'le64_gcc4.9',
                        'le_oe_gcc6.4', 'le64_oe_gcc6.4']
  -d, --debug           Set to turn on debug log
  -s SLEEP, --sleep SLEEP
                        Set number of seconds to sleep between runs e.g. 20
                        seconds
  -b USERBUFFER_MODE, --userbuffer_mode USERBUFFER_MODE
                        [EXPERIMENTAL] Enable user buffer mode, default to
                        float, can be tf8exact0
  -p PERFPROFILE, --perfprofile PERFPROFILE
                        Set the benchmark operating mode (balanced, default,
                        sustained_high_performance, high_performance,
                        power_saver, system_settings)
  -l PROFILINGLEVEL, --profilinglevel PROFILINGLEVEL
                        Set the profiling level mode (off, basic, detailed).
                        Default is basic. Basic profiling only applies to DSP
                        runtime.
  -json, --generate_json
                        Set to produce json output.
  -cache, --enable_init_cache
                        Enable init caching mode to accelerate the network
                        building process. Defaults to disable.
</pre><hr/>
 <h1><a class="anchor" id="tools_snpe-caffe-to-dlc"></a>
snpe-caffe-to-dlc</h1>
<p>snpe-caffe-to-dlc converts a Caffe model into an SNPE DLC file.</p>
<pre class="fragment">usage: snpe-caffe-to-dlc [-h] -c CAFFE_TXT [-b CAFFE_BIN] [-d DLC]
                         [--omit_preprocessing]
                         [--encoding {argb32,rgba,nv21,bgr}]
                         [--input_size WIDTH HEIGHT]
                         [--model_version MODEL_VERSION]
                         [--disable_batchnorm_folding]
                         [--in_layer INPUT_LAYERS]
                         [--in_type {default,image,opaque}]
                         [--validation_target RUNTIME_TARGET PROCESSOR_TARGET]
                         [--strict] [--verbose]

Script to convert caffe protobuf configuration into a DLC file.

required arguments:
  -c CAFFE_TXT, --caffe_txt CAFFE_TXT
                        Input caffe proto txt configuration file

optional arguments:
  -b CAFFE_BIN, --caffe_bin CAFFE_BIN
                        Input caffe binary file containing the weight data
  -d DLC, --dlc DLC     Output DLC file containing the model. If not
                        specified, the data will be written to a file with
                        same name as the caffetxt file with a .dlc extension
  --copyright_file COPYRIGHT_FILE
                        Path to copyright file. If provided, the content of
                        the file will be added to the dlc.
  --omit_preprocessing  If specified, converter will disable preprocessing
                        specified by a data layer transform_param or any
                        preprocessing command line options
  --encoding {argb32,rgba,nv21,bgr}
                        Image encoding of the source images. Default is bgr if
                        not specified
  --input_size WIDTH HEIGHT
                        Dimensions of the source images for scaling, if
                        different from the network input.
  --model_version MODEL_VERSION
                        User-defined ASCII string to identify the model, only
                        first 64 bytes will be stored
  --disable_batchnorm_folding
                        If not specified, converter will try to fold batchnorm
                        into previous convolution layer
  --in_layer INPUT_LAYERS
                        Name of the input layer
  --in_type {default,image,opaque}
                        Type of data expected by input layer. Type is default
                        if not specified.
  --validation_target RUNTIME_TARGET PROCESSOR_TARGET
                        A combination of processor and runtime target against
                        which model will be validated.Choices for
                        RUNTIME_TARGET: {cpu, gpu, dsp}.Choices for
                        PROCESSOR_TARGET: {snapdragon_801, snapdragon_820,
                        snapdragon_835}.If not specified, will validate model
                        against {snapdragon_820, snapdragon_835} across all
                        runtime targets.
  --strict              If specified, will validate in strict mode whereby
                        model will not be produced if it violates constraints
                        of the specified validation target.If not specified,
                        will validate model in permissive mode against the
                        specified validation target.
  --verbose             Verbose printing</pre><p>Examples of using this script can be found in <a class="el" href="model_conv_caffe.html#conversion_caffe">Converting Models from Caffe to SNPE</a>.<br />
 <br />
 Additional details:<br />
 </p><ul>
<li>
<em>omit_preprocessing argument:</em> <ul>
<li>
Disables all preprocessing. If specified, all input preprocessing is bypassed. The following are bypassed: <ul>
<li>
"encoding" command line option to the converter. </li>
<li>
"input_size" command line option to the converter. </li>
<li>
Any transform_param specified in the data layer (input layer) of the prototxt. </li>
</ul>
</li>
<li>
This argument is optional. If not provided the converter will process the preprocessing command line options as well as any applicable layer transform_param encountered in the prototxt. </li>
</ul>
</li>
</ul>
<ul>
<li>
<em>encoding argument:</em> <ul>
<li>
Specifies the encoding type of input images. </li>
<li>
A preprocessing layer is added to the network to convert input images from the specified encoding to BGR, the encoding used by Caffe. </li>
<li>
The encoding preprocessing layer can be seen when using snpe-dlc-info. </li>
<li>
Allowed options are: <ul>
<li>
<b>argb32</b>: The ARGB32 format consists of 4 bytes per pixel: one byte for Red, one for Green, one for Blue and one for the alpha channel. The alpha channel is ignored. For little endian CPUs, the byte order is BGRA. For big endian CPUs, the byte order is ARGB.  </li>
<li>
<b>rgba</b>: The RGBA format consists of 4 bytes per pixel: one byte for Red, one for Green, one for Blue and one for the alpha channel. The alpha channel is ignored. The byte ordering is endian independent and is always RGBA byte order.  </li>
<li>
<b>nv21</b>: NV21 is the Android version of YUV. The Chrominance is down sampled and has a sub sampling ratio of 4:2:0. Note that this image format has 3 channels, but the U and V channels are subsampled. For every four Y pixels there is one U and one V pixel.  </li>
<li>
<b>bgr</b>: The BGR format consists of 3 bytes per pixel: one byte for Red, one for Green and one for Blue. The byte ordering is endian independent and is always BGR byte order.  </li>
</ul>
</li>
<li>
This argument is optional. If omitted then input image encoding is assumed to be BGR and no preprocessing layer is added. </li>
<li>
See <a class="el" href="input_preprocessing.html">Image Preprocessing</a> for more details. </li>
</ul>
</li>
</ul>
<ul>
<li>
<em>input_size argument:</em> <ul>
<li>
Specifies the size of the source image for scaling. </li>
<li>
A preprocessing layer is added to the network to scale input images from the specified size to the size required by the network. </li>
<li>
The preprocessing scaling layer can be seen when using snpe-dlc-info. </li>
<li>
This argument is optional. If omitted then no preprocessing scaling layer is added. </li>
<li>
See <a class="el" href="input_preprocessing.html">Image Preprocessing</a> for more details. </li>
</ul>
</li>
</ul>
<ul>
<li>
<em>disable_batchnorm_folding argument:</em> <ul>
<li>
The disable batchnorm folding argument allows the user to turn off the optimization that folds batchnorm and batchnorm + scaling layers into previous convolution layers when possible. </li>
<li>
This argument is optional. If omitted then the converter will fold batchnorm and batchnorm + scaling layers into previous convolution layers wherever possible as an optimization. When this occurs the names of the folded batchnorm and scale layers are concatenated to the convolution layer it was folded into. <ul>
<li>
For example: if batchnorm layer named 'bn' and scale layer named 'scale' are folded into a convolution layer named 'conv', the resulting dlc will show the convolution layer to be named 'conv.bn.scale'. </li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>
<em>in_layer argument:</em> <ul>
<li>
Specifies the name of the input layer associated to the in_type argument that follows it. </li>
<li>
This argument can be passed more than once if you want to specify the expected data type of two or more input layers. </li>
<li>
in_layer argument needs to be followed by in_type argument. </li>
<li>
This argument is optional. If omitted for a certain input layer than the expected data type will be the default. </li>
</ul>
</li>
</ul>
<ul>
<li>
<em>in_type argument:</em> <ul>
<li>
Specifies the expected data type for a certain input layer name specified by the in_layer argument preceding it. </li>
<li>
This argument can be passed more than once if you want to specify the expected data type of two or more input layers. </li>
<li>
in_type argument needs to be preceded by in_layer argument. </li>
<li>
This argument is optional. If omitted for a certain input layer than the expected data type will be the default. </li>
<li>
Allowed options are: <ul>
<li>
<b>default</b>: Specifies that the input contains floating-point values.  </li>
<li>
<b>image</b>: Specifies that the input contains floating-point values that are all integers in the range 0..255.  </li>
<li>
<b>opaque</b>: Specifies that the input contains floating-point values that should be passed to the selected runtime without modification. <br />
 For example an opaque tensor is passed directly to the DSP without quantization.  </li>
</ul>
</li>
<li>
For example: "--in_layer data --in_type image --in_layer roi --in_type opaque". </li>
</ul>
</li>
</ul>
<hr/>
 <h1><a class="anchor" id="tools_snpe-caffe2-to-dlc"></a>
snpe-caffe2-to-dlc</h1>
<p>snpe-caffe2-to-dlc converts a Caffe2 model into an SNPE DLC file.</p>
<pre class="fragment">usage: snpe-caffe2-to-dlc [-h] -p PREDICT_NET -e EXEC_NET -i INPUT_DIM
                          INPUT_DIM [-d DLC] [--enable_preprocessing]
                          [--encoding {argb32,rgba,nv21,bgr}]
                          [--opaque_input [OPAQUE_INPUT [OPAQUE_INPUT ...]]]
                          [--model_version MODEL_VERSION]
                          [--reorder_list REORDER_LIST [REORDER_LIST ...]]
                          [--verbose]

Script to convert caffe2 networks into a DLC file.

optional arguments:
  -h, --help            show this help message and exit

required arguments:
  -p PREDICT_NET, --predict_net PREDICT_NET
                        Input caffe2 binary network definition protobuf
  -e EXEC_NET, --exec_net EXEC_NET
                        Input caffe2 binary file containing the weight data
  -i INPUT_DIM INPUT_DIM, --input_dim INPUT_DIM INPUT_DIM
                        The names and dimensions of the network input layers
                        specified in the format "input_name" B,C,H,W. Ex "data"
                        1,3,224,224. Note that the quotes should always be
                        included in order to handle special characters,
                        spaces, etc. For multiple inputs specify multiple
                        --input_dim on the command line like: --input_dim
                        "data1" 1,3,224,224 --input_dim "data2" 1,3,50,100 We
                        currently assume that all inputs have 4 dimensions.

optional arguments:
  -d DLC, --dlc DLC     Output DLC file containing the model. If not
                        specified, the data will be written to a file with
                        same name and location as the predict_net file with a
                        .dlc extension
  --copyright_file COPYRIGHT_FILE
                        Path to copyright file. If provided, the content of
                        the file will be added to the dlc.
  --enable_preprocessing
                        If specified, the converter will enable image mean
                        subtraction and cropping specified by ImageInputOp. Do
                        NOT enable if there is not a ImageInputOp present in
                        the Caffe2 network.
  --encoding {argb32,rgba,nv21,bgr}
                        Image encoding of the source images. Default is bgr if
                        not specified
  --opaque_input [OPAQUE_INPUT [OPAQUE_INPUT ...]]
                        A space separated list of input blob names which
                        should be treated as opaque (non-image) data. These
                        inputs will be consumed as-is by SNPE. Any input blob
                        not listed will be assumed to be image data.
  --model_version MODEL_VERSION
                        User-defined ASCII string to identify the model, only
                        first 64 bytes will be stored
  --reorder_list REORDER_LIST [REORDER_LIST ...]
                        A list of external inputs or outputs that SNPE should
                        automatically reorder to match the specified Caffe2
                        channel ordering. Note that this feature is only
                        enabled for the GPU runtime.
  --verbose             Verbose printing</pre><hr/>
 <h1><a class="anchor" id="tools_snpe-diagview"></a>
snpe-diagview</h1>
<p>snpe-diagview loads a DiagLog file generated by snpe-net-run whenever it operates on input tensor data. The DiagLog file contains timing information information for each layer as well as the entire forward propagate time. If the run uses an input list of input tensors, the timing info reported by snpe-diagview is an average over the entire input set.</p>
<p>The snpe-net-run generates a file called "SNPEDiag_0.log", "SNPEDiag_1.log" ... , "SNPEDiag_n.log", where n corresponds to the nth iteration of the snpe-net-run execution.</p>
<pre class="fragment">usage: snpe-diagview --input_log DIAG_LOG [-h] [--output CSV_FILE]

Reads a diagnostic log and output the contents to stdout

required arguments:
  --input_log     DIAG_LOG
                Diagnostic log file (required)
optional arguments:
  --output        CSV_FILE
                Output CSV file with all diagnostic data (optional)</pre><p> <br />
</p>
<hr/>
 <h1><a class="anchor" id="tools_snpe-dlc-info"></a>
snpe-dlc-info</h1>
<p>snpe-dlc-info outputs layer information from a DLC file, which provides information about the network model.</p>
<pre class="fragment">usage: snpe-dlc-info [-h] -i INPUT_DLC [-s SAVE]

required arguments:
  -i INPUT_DLC, --input_dlc INPUT_DLC
                        path to a DLC file

optional arguments:
  -s SAVE, --save SAVE
                        Save the output to a csv file. Specify a target file path.</pre><p> <br />
</p>
<hr/>
 <h1><a class="anchor" id="tools_snpe-dlc-diff"></a>
snpe-dlc-diff</h1>
<p>snpe-dlc-diff compares two DLCs and outputs some of the following differences in them in a tabular format:</p><ul>
<li>unique layers between the two DLCs</li>
<li>parameter differences in common layers</li>
<li>differences in dimensions of buffers associated with common layers</li>
<li>weight differences in common layers</li>
<li>unique records between the two DLCs (currently checks for AIP records only)</li>
</ul>
<pre class="fragment">usage: snpe-dlc-diff [-h] -i1 INPUT_DLC_ONE -i2 INPUT_DLC_TWO [-c] [-l] [-p]
                     [-d] [-w] [-o] [-a] [-x] [-s SAVE]

required arguments:
  -i1 INPUT_DLC_ONE, --input_dlc_one INPUT_DLC_ONE
                        path to the first dl container archive
  -i2 INPUT_DLC_TWO, --input_dlc_two INPUT_DLC_TWO
                        path to the second dl container archive

optional arguments:
  -c, --copyrights      compare copyrights between models
  -l, --layers          compare unique layers between models
  -p, --parameters      compare parameter differences between identically
                        named layers
  -d, --dimensions      compare dimension differences between identically
                        named layers
  -w, --weights         compare layer-by-layer weight differences between
                        models (with same architecture)
  -o, --outputs         compare differences in layer outputs between models
  -a, --aix             compare AIP records differences in Models (deprecated,
                        use option -x/--hta)
  -x, --hta             compare AIP records differences in Models
  (A combination of the above arguments can be used to view multiple tables. Eg: -lp outputs layer and parameter difference table)
  -s SAVE, --save SAVE  Save the output to a csv file. Specify a target file
                        path.
  -h, --help            show this help message and exit</pre><p> <br />
</p>
<hr/>
 <h1><a class="anchor" id="tools_snpe-dlc-viewer"></a>
snpe-dlc-viewer</h1>
<p>snpe-dlc-viewer visualizes the network structure of a DLC in a web browser.</p>
<pre class="fragment">usage: snpe-dlc-viewer [-h] -i INPUT_DLC [-s]

required arguments:
  -i INPUT_DLC, --input_dlc INPUT_DLC
                        Path to a DLC file

optional arguments:
  -s, --save            Save HTML file. Specify a file name and/or target save path
  -h, --help            Shows this help message and exits</pre><p>Additional details:</p>
<p><br />
The DLC viewer tool renders the specified network DLC in HTML format that may be viewed on a web browser. <br />
On installations that support a native web browser a browser instance is opened on which the network is automatically rendered. <br />
Users can optionally save the HTML content anywhere on their systems and open on a chosen web browser independently at a later time.</p>
<ul>
<li>
Features: <ul>
<li>
Graph-based representation of network model with nodes depicting layers and edges depicting buffer connections. </li>
<li>
Colored legend to indicate layer types. </li>
<li>
Zoom and drag options available for ease of visualization. </li>
<li>
Tool-tips upon mouse hover to describe detailed layer parameters. </li>
<li>
Sections showing metadata from DLC records </li>
</ul>
</li>
</ul>
<ul>
<li>
Supported browsers: <ul>
<li>
Google Chrome </li>
<li>
Firefox </li>
<li>
Internet Explorer on Windows </li>
<li>
Microsoft Edge Browser on Windows </li>
<li>
Safari on Mac </li>
</ul>
</li>
</ul>
<hr/>
 <h1><a class="anchor" id="tools_snpe-dlc-quantize"></a>
snpe-dlc-quantize</h1>
<p>snpe-dlc-quantize converts non-quantized DLC models into quantized DLC models.</p>
<pre class="fragment">Command Line Options:
  [ -h,  --help ]       Displays this help message.
  [ --version ]         Displays version information.
  [ --verbose ]         Enable verbose user messages.
  [ --quiet ]           Disables some user messages.
  [ --silent ]          Disables all but fatal user messages.
  [ --debug=&lt;val&gt; ]     Sets the debug log level.
  [ --debug1 ]          Enables level 1 debug messages.
  [ --debug2 ]          Enables level 2 debug messages.
  [ --debug3 ]          Enables level 3 debug messages.
  [ --log-mask=&lt;val&gt; ]  Sets the debug log mask to set the log level for one or more areas.
                        Example: ".*=USER_ERROR, .*=INFO, NDK=DEBUG2, NCC=DEBUG3"
  [ --log-file=&lt;val&gt; ]  Overrides the default name for the debug log file.
  [ --log-dir=&lt;val&gt; ]   Overrides the default directory path where debug log files are written.
  [ --log-file-include-hostname ]
                        Appends the name of this host to the log file name.
  --input_dlc=&lt;val&gt;     Path to the dlc container containing the model for which fixed-point encoding
                        metadata should be generated. This argument is required.
  --input_list=&lt;val&gt;    Path to a file specifying the trial inputs. This file should be a plain text file,
                        containing one or more absolute file paths per line. These files will be taken to constitute
                        the trial set. Each path is expected to point to a binary file containing one trial input
                        in the 'raw' format, ready to be consumed by SNPE without any further modifications.
                        This is similar to how input is provided to snpe-net-run application.
  [ --no_weight_quantization ]
                        Generate and add the fixed-point encoding metadata but keep the weights in
                        floating point. This argument is optional.
  [ --output_dlc=&lt;val&gt; ]
                        Path at which the metadata-included quantized model container should be written.
                        If this argument is omitted, the quantized model will be written at &lt;unquantized_model_name&gt;_quantized.dlc.
  [ --use_enhanced_quantizer ]
                        Use the enhanced quantizer feature when quantizing the model.  Regular quantization determines the range using the actual
                        values of min and max of the data being quantized.  Enhanced quantization uses an algorithm to determine optimal range.  It can be
                        useful for quantizing models that have long tails in the distribution of the data being quantized. Valid single argument options are "weights"
                        for quantized weights only, "activations" for quantized activations only, or leave blank to quantize both weights and activations.
  [ --enable_aix ]      Pack HTA information in quantized DLC.\n
                        This option is deprecated, will be removed in a future release.
  [ --aix_partitions=&lt;val&gt; ]
                        Specify a single subnet partition to run on HTA.
                        Partitions to be specified with start and end layer IDs, like 0-20.\n
                        This option is deprecated, will be removed in a future release.
  [ --enable_hta ]      Pack HTA information in quantized DLC.\n
  [ --hta_partitions=&lt;val&gt; ]
                        Specify a single subnet partition to run on HTA.
                        Partitions to be specified with start and end layer IDs, like 0-20.\n

Description:
Generate 8 bit TensorFlow style fixed point weight and activations encodings for a floating point SNPE model.</pre><p><br />
 Additional details: <br />
 </p><ul>
<li>
For specifying input_list, refer to <a class="el" href="tools.html#tools_snpe-net-run_input_list">input_list argument</a> in <a class="el" href="tools.html#tools_snpe-net-run">snpe-net-run</a> for supported input formats (in order to calculate output activation encoding information for all layers, <b>do not</b> include the line which specifies desired outputs). </li>
<li>
The tool requires the batch dimension of the DLC input file to be set to 1 during the original model conversion step. </li>
<li>
An example of quantization using snpe-dlc-quantize can be found in the C++ Tutorial section:<a class="el" href="tutorial_inceptionv3.html">Running the Inception v3 Model</a>. For details on quantization see <a class="el" href="quantized_models.html">Quantized vs Non-Quantized Models</a>. </li>
<li>
Using snpe-dlc-quantize is mandatory for running on HTA. See <a class="el" href="hta_support.html">Adding HTA sections</a> </li>
</ul>
<p><br />
</p>
<hr/>
 <h1><a class="anchor" id="tools_snpe-tensorflow-to-dlc"></a>
snpe-tensorflow-to-dlc</h1>
<p>snpe-tensorflow-to-dlc converts a TensorFlow model into an SNPE DLC file.</p>
<pre class="fragment">usage: snpe-tensorflow-to-dlc [-h] --graph GRAPH -i INPUT_NAME INPUT_DIM
                              --out_node OUT_NODE [--dlc DLC]
                              [--model_version MODEL_VERSION]
                              [--in_type {default,image}]
                              [--allow_unconsumed_nodes] [--verbose]

Script to convert a TensorFlow graph into a DLC file.

required arguments:
  --graph GRAPH         Path to TensorFlow graph def (.pb saved as binary) or
                        graph meta (.meta) file.
  -i INPUT_NAME INPUT_DIM, --input_dim INPUT_NAME INPUT_DIM
                        The names and dimensions of the network input layers
                        specified in the format "input_name" comma-separated-
                        dimensions, for example: "data" 1,224,224,3. Note that
                        the quotes should always be included in order to handle
                        special characters, spaces, etc. For multiple inputs
                        specify multiple --input_dim on the command line like:
                        --input_dim "data1" 1,224,224,3 --input_dim "data2" 1,50,100,3.
  --out_node OUT_NODE   Name of the graph's output node.

optional arguments:
  --dlc DLC             Path to DLC file to be generated.
  --copyright_file COPYRIGHT_FILE
                        Path to copyright file. If provided, the content of
                        the file will be added to the dlc.
  --model_version MODEL_VERSION
                        User-defined ASCII string to identify the model, only
                        first 64 bytes will be stored
  --in_type {default,image}
                        Type of data expected by input layer. Type is default
                        if not specified.
  --allow_unconsumed_nodes
                        Uses a relaxed graph node to layer mapping algorithm
                        which may not use all graph nodes during conversion
                        while retaining structural integrity.
  --verbose             Verbose printing</pre><p>Examples of using this script can be found in Converting Models from TensorFlow to SNPE.<br />
 <br />
 Additional details:<br />
 </p><ul>
<li>
<em>graph argument:</em> <ul>
<li>
The converter supports either a single frozen graph .pb file or a pair of graph meta and checkpoint files. </li>
<li>
If you are using the <a href="https://www.tensorflow.org/versions/r0.10/api_docs/python/state_ops.html#Saver">TensorFlow Saver</a> to save your graph during training, 3 files will be generated as described below: <ol>
<li>
&lt;model-name&gt;.meta </li>
<li>
&lt;model-name&gt; </li>
<li>
checkpoint </li>
</ol>
</li>
<li>
The converter --graph option specifies the path to the graph meta file. The converter will also use the checkpoint file to read the graph nodes parameters during conversion. The checkpoint file must have the same name without the .meta suffix. </li>
</ul>
</li>
<li>
<em>input_dim argument:</em> <ul>
<li>
Specifies the input dimensions of the graph's input node. The converter expects a comma separated list with the model's input dimensions.. </li>
<li>
<b>Multiple Inputs</b> <ul>
<li>
Networks with multiple inputs must provide both --<b>in_node</b> and --<b>input_dim</b>, one for each input node. </li>
</ul>
</li>
</ul>
</li>
<li>
<em>in_node argument:</em> <ul>
<li>
Specifies the input node name. The converter requires a node name as input from which it will create an input layer by using the node output tensor dimensions. When defining a graph, there is typically a placeholder name used as input during training in the graph. The placeholder tensor name is the name you must use as the argument. It is also possible to use other types of nodes as input, however the node used as input will not be used as part of a layer other than the input layer. </li>
<li>
<b>Multiple Inputs</b> <ul>
<li>
Networks with multiple inputs must provide both --<b>in_node</b> and --<b>input_dim</b>, one for each input node. </li>
</ul>
</li>
</ul>
</li>
<li>
<em>out_node argument:</em> <ul>
<li>
<p class="startli">The name of the last node in your TensorFlow graph which will represent the output layer of your network.</p>
<p class="endli"></p>
</li>
<li>
<b>Multiple Outputs</b> <ul>
<li>
Networks with multiple outputs must provide several --<b>out_node</b> arguments, one for each output node. </li>
</ul>
</li>
</ul>
</li>
<li>
<em>dlc argument:</em> <ul>
<li>
Specifies the output DLC file name. </li>
<li>
This argument is optional. If not provided the converter will create a DLC file file with the same name as the graph file name, with a .dlc file extension. </li>
</ul>
</li>
</ul>
<hr/>
 <h1><a class="anchor" id="tools_snpe-onnx-to-dlc"></a>
snpe-onnx-to-dlc</h1>
<p>snpe-onnx-to-dlc converts a serialized ONNX model into a SNPE DLC file.</p>
<pre class="fragment">usage: snpe-onnx-to-dlc [-h] --model_path MODEL_PATH [--dlc_path DLC_PATH]
                        [--encoding ENCODING ENCODING]
                        [--disable_batchnorm_folding] [--debug]

optional arguments:
  -h, --help            show this help message and exit
  --model_path MODEL_PATH, -m MODEL_PATH
                        Path to the source ONNX model.
  --dlc_path DLC_PATH, -d DLC_PATH
                        Path where the converted DLC model should be saved.
  --copyright_file COPYRIGHT_FILE
                        Path to copyright file. If provided, the content of
                        the file will be added to the dlc.
  --encoding ENCODING ENCODING
                        Set the image encoding for an input buffer. This
                        should be specifed in the format "--encoding &lt;input
                        name&gt; &lt;encoding&gt;", where encoding is one of: "argb32",
                        "rgba", "nv21", "opaque", or "bgr". The default
                        encoding for all inputs not so described is "bgr".
                        "opaque" inputs will be interpreted as-is by SNPE, and
                        not subject to order transformations.
  --disable_batchnorm_folding
                        If not specified, converter will try to fold batchnorm
                        into previous convolution layer
  --debug               Run the converter in debug mode</pre><p>For more information, see <a class="el" href="model_conv_onnx.html">ONNX Model Conversion</a></p>
<hr/>
 <h1><a class="anchor" id="tools_snpe-platform-validator"></a>
snpe-platform-validator</h1>
<pre class="fragment">DESCRIPTION:
------------
snpe-platform-validator checks the SNPE compatibility/capability of a device. This tool runs on the device,
rather than on the host, and requires a few additional files to be pushed to the device besides its own executable.
Additional details below.


REQUIRED ARGUMENTS:
-------------------
  --runtime &lt;RUNTIME&gt;   Specify the runtime to validate. &lt;RUNTIME&gt; : gpu, dsp, aip, all.

OPTIONAL ARGUMENTS:
-------------------
  --coreVersion         Query the runtime core descriptor.
  --libVersion          Query the runtime core library API.
  --testRuntime         Run diagnostic tests on the specified runtime.
  --targetPath &lt;DIR&gt;    The directory to save output on the device. Defaults to /data/local/tmp/platformValidator/output.
  --debug               Turn on verbose logging.
  --help                Show this help message.</pre><p> Additional details:<br />
 </p><ul>
<li>
<em>File needed to be pushed to device:</em> <ul>
<li>
<pre class="fragment">   bin/snpe-platform-validator
   lib/libcalculator_domains.so
   lib/libcalculator.so
   lib/libsymphonypower.so
   lib/libsnpe_adsp.so
   lib/libsnpe_dsp_domains.so
   lib/libsnpe_dsp_domains_v2.so
   lib/libsnpe_dsp_domains_system.so
   lib/dsp/libcalculator_skel.so
   lib/dsp/libsnpe_dsp_domains_skel.so
   lib/dsp/libsnpe_dsp_skel.so
   lib/dsp/libsnpe_dsp_v65_domains_v2_skel.so

   example: for pushing arm-android-clang6.0 variant to /data/local/tmp/platformValidator

   adb push $SNPE_ROOT/bin/arm-android-clang6.0/snpe-platform-validator /data/local/tmp/platformValidator/bin/snpe-platform-validator
   adb push $SNPE_ROOT/lib/arm-android-clang6.0 /data/local/tmp/platformValidator/lib
   adb push $SNPE_ROOT/lib/dsp /data/local/tmp/platformValidator/dsp</pre>  </li>
</ul>
</li>
</ul>
<hr/>
 <h1><a class="anchor" id="tools_snpe-throughput-net-run"></a>
snpe-throughput-net-run</h1>
<p>snpe-throughput-net-run concurrently runs multiple instances of SNPE for a certain duration of time and measures inference throughput. Each instance of SNPE can have its own model, designated runtime and performance profile. Please note that the "--duration" parameter is common for all instances of SNPE created.</p>
<pre class="fragment">DESCRIPTION:
------------
Example application demonstrating how to load concurrent SNPE objects
using the SNPE C++ API.


REQUIRED ARGUMENTS:
-------------------
  --container  &lt;FILE&gt;   Path to the DL container containing the network.
  --duration   &lt;VAL&gt;    Duration of time (in seconds) to run network execution.
  --use_cpu             Use the CPU runtime for SNPE.
  --use_gpu             Use the GPU float32 runtime for SNPE.
  --use_gpu_fp16        Use the GPU float16 runtime for SNPE.
  --use_dsp             Use the DSP fixed point runtime for SNPE.
  --perf_profile &lt;VAL&gt;  Specifies perf profile to set. Valid settings are "balanced" , "default" , "high_performance" ,
                        "sustained_high_performance" , "burst" , "power_saver" and "system_settings".
                        NOTE: "balanced" and "default" are the same.  "default" is being deprecated in the future.
  --use_fxp_cpu         Use the CPU fixed point runtime for SNPE.
  --use_aip             Use the AIP fixed point runtime for SNPE


OPTIONAL ARGUMENTS:
-------------------
  --debug               Specifies that output from all layers of the network
                        will be saved.
  --storage_dir &lt;DIR&gt;   The directory to store SNPE metadata files
  --version             Show SNPE Version Number.
  --iterations &lt;VAL&gt;    Number of times to iterate through entire input list
  --verbose             Print more debug information.
  --enable_cpu_fallback Enables cpu fallback functionality. Defaults to disable mode.
  --help                Show this help message.</pre> <hr/>
<h1><a class="anchor" id="tools_snpe-platform-validator-py"></a>
snpe-platform-validator-py</h1>
<pre class="fragment">DESCRIPTION:
------------
snpe-platform-validator-py checks the SNPE compatibility/capability of a device. The output is saved in a CSV file in the
"Output" directory, in a csv format. Basic logs are also displayed on the console.

REQUIRED ARGUMENTS:
-------------------
  --runtime &lt;RUNTIME&gt;      Specify the runtime to validate. &lt;RUNTIME&gt; : gpu, dsp, aip, all.
  --directory &lt;ARTIFACTS&gt;  Path to the root of the unpacked SDK directory containing the executable and library files.

OPTIONAL ARGUMENTS:
-------------------
  --buildVariant &lt;VARIANT&gt;      Specify the build variant (e.g: arm-android-clang6.0(default), aarch64-android-clang6.0) to be validated.
  --deviceId                    Uses the device for running the adb command. Defaults to first device in the adb devices list.
  --coreVersion                 Outputs the version of the runtime that is present on the target.
  --libVersion                  Outputs the library version of the runtime that is present on the target.
  --testRuntime                 Runs a small program on the runtime and Checks if SNPE is supported for runtime.
  --targetPath &lt;PATH&gt;           The path to be used on the device. Defaults to /data/local/tmp/platformValidator
                                NOTE that this directory will be deleted before proceeding with validation.
  --remoteHost &lt;REMOTEHOST&gt;     Run on remote host through remote adb server. Defaults to localhost.
  --debug                       Set to turn on debug log.</pre><hr/>
 </div></div><!-- contents -->
</div><!-- doc-content -->
<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  Copyright (c) 2016-2018 Qualcomm Technologies, Inc.
  All Rights Reserved.
  Confidential and Proprietary - Qualcomm Technologies, Inc.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 -->
<!-- start footer part -->
<div id="nav-path" class="navpath" font-size:small;><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">
      <p align="right">
        80-NL315-14 A <br>
        MAY CONTAIN U.S. AND INTERNATIONAL EXPORT CONTROLLED INFORMATION
        <!--If the Controlled Distribution statement is to be included, uncomment below:-->
        <!--<b>Controlled Distribution - DO NOT COPY</b>-->
        <img class="footer" width:5%; alt="QTI Logo" src="images/QTI_Logo.png" />
      </p>
    </li>
  </ul>
</div>
</body>
</html>
